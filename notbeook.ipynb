{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chess\n",
    "\n",
    "from env import *\n",
    "env = Chess_env()\n",
    "obs = env.reset()\n",
    "\n",
    "from deepQ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(100):\n",
    "    action = np.random.choice(list(env.board.legal_moves))\n",
    "    _,_,done,_ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" baseProfile=\"tiny\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light lastmove g6\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light lastmove h7\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(285, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 285)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 285)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(105, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 240)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(60, 195)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 150)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(60, 150)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 150)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(240, 150)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 150)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(330, 150)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 105)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(60, 105)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(285, 105)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 60)\" /></svg>",
      "text/plain": [
       "Board('8/R7/pr4b1/Pk1p1npQ/1n6/2rP3P/2P1K3/6R1 w - - 1 52')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.board_feat.translate(env.board.fen())\n",
    "env.board_feat.board[:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [2., 1., 0., 1., 0., 0., 1., 0.],\n",
       "       [2., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [2., 1., 2., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 2., 2., 1., 1., 0.],\n",
       "       [1., 0., 0., 2., 0., 1., 2., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env._next_observation()\n",
    "\n",
    "np.sum((np.sum(obs[0], axis=0)*(1-env.board_feat.board)),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 8, 13)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 3, 3, 32)          3776      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 64)          8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 1, 128)         32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 62,336\n",
      "Trainable params: 61,888\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = DeepQ(env, dropout_rate=0.1, L2_reg=10)\n",
    "model.head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = env.create_pretraining_dataset(100,400,0.02)\n",
    "X_test,y_test = env.create_pretraining_dataset(50,400,0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 completed, train loss : 0.19174160668626428, test loss : 0.18814902007579803\n",
      "Epoch : 1 completed, train loss : 0.17524525336921215, test loss : 0.18530720472335815\n",
      "Epoch : 2 completed, train loss : 0.18893112055957317, test loss : 0.18585023283958435\n",
      "Epoch : 3 completed, train loss : 0.1797143230214715, test loss : 0.18590222299098969\n",
      "Epoch : 4 completed, train loss : 0.1738738319836557, test loss : 0.18428459763526917\n",
      "Epoch : 5 completed, train loss : 0.16650461871176958, test loss : 0.18548239767551422\n",
      "Epoch : 6 completed, train loss : 0.16861938405781984, test loss : 0.18573974072933197\n",
      "Epoch : 7 completed, train loss : 0.162386370357126, test loss : 0.18889713287353516\n",
      "Epoch : 8 completed, train loss : 0.16285498021170497, test loss : 0.18917451798915863\n",
      "Epoch : 9 completed, train loss : 0.16241134703159332, test loss : 0.18964911997318268\n",
      "Epoch : 10 completed, train loss : 0.15698310686275363, test loss : 0.19020706415176392\n",
      "Epoch : 11 completed, train loss : 0.157825184520334, test loss : 0.19517654180526733\n",
      "Epoch : 12 completed, train loss : 0.15547169372439384, test loss : 0.19370628893375397\n",
      "Epoch : 13 completed, train loss : 0.15474260691553354, test loss : 0.19488759338855743\n",
      "Epoch : 14 completed, train loss : 0.14522262942045927, test loss : 0.19284677505493164\n",
      "Epoch : 15 completed, train loss : 0.14987404178828, test loss : 0.1965557187795639\n",
      "Epoch : 16 completed, train loss : 0.14298398373648524, test loss : 0.19869624078273773\n",
      "Epoch : 17 completed, train loss : 0.1412858827970922, test loss : 0.19759026169776917\n",
      "Epoch : 18 completed, train loss : 0.14331198250874877, test loss : 0.2013271003961563\n",
      "Epoch : 19 completed, train loss : 0.13443064922466874, test loss : 0.20052814483642578\n",
      "Epoch : 20 completed, train loss : 0.13842928037047386, test loss : 0.20315226912498474\n",
      "Epoch : 21 completed, train loss : 0.13006465742364526, test loss : 0.20244669914245605\n",
      "Epoch : 22 completed, train loss : 0.13107543950900435, test loss : 0.20311374962329865\n",
      "Epoch : 23 completed, train loss : 0.12864869786426425, test loss : 0.20525942742824554\n",
      "Epoch : 24 completed, train loss : 0.12803274439647794, test loss : 0.20894098281860352\n",
      "Epoch : 25 completed, train loss : 0.13242073683068156, test loss : 0.20596261322498322\n",
      "Epoch : 26 completed, train loss : 0.12901351926848292, test loss : 0.21080565452575684\n",
      "Epoch : 27 completed, train loss : 0.12089882418513298, test loss : 0.2101019024848938\n",
      "Epoch : 28 completed, train loss : 0.12268987810239196, test loss : 0.21096695959568024\n",
      "Epoch : 29 completed, train loss : 0.12324498454108834, test loss : 0.21048878133296967\n",
      "Epoch : 30 completed, train loss : 0.11276382626965642, test loss : 0.211521714925766\n",
      "Epoch : 31 completed, train loss : 0.11108064092695713, test loss : 0.21540135145187378\n",
      "Epoch : 32 completed, train loss : 0.11305825831368566, test loss : 0.21322974562644958\n",
      "Epoch : 33 completed, train loss : 0.10939477244392037, test loss : 0.2156994491815567\n",
      "Epoch : 34 completed, train loss : 0.11070886300876737, test loss : 0.2191210389137268\n",
      "Epoch : 35 completed, train loss : 0.10647885408252478, test loss : 0.2211637943983078\n",
      "Epoch : 36 completed, train loss : 0.10390590829774737, test loss : 0.2177591174840927\n",
      "Epoch : 37 completed, train loss : 0.10859843622893095, test loss : 0.21759046614170074\n",
      "Epoch : 38 completed, train loss : 0.10267431428655982, test loss : 0.22114676237106323\n",
      "Epoch : 39 completed, train loss : 0.0986690353602171, test loss : 0.22354739904403687\n",
      "Epoch : 40 completed, train loss : 0.1011728085577488, test loss : 0.2225847691297531\n",
      "Epoch : 41 completed, train loss : 0.10111571568995714, test loss : 0.22062106430530548\n",
      "Epoch : 42 completed, train loss : 0.1016570245847106, test loss : 0.22435925900936127\n",
      "Epoch : 43 completed, train loss : 0.09652996738441288, test loss : 0.2242754101753235\n",
      "Epoch : 44 completed, train loss : 0.09312464017421007, test loss : 0.22432594001293182\n",
      "Epoch : 45 completed, train loss : 0.09061806835234165, test loss : 0.2262168824672699\n",
      "Epoch : 46 completed, train loss : 0.09156350744888186, test loss : 0.22423943877220154\n",
      "Epoch : 47 completed, train loss : 0.09066150360740721, test loss : 0.22639156877994537\n",
      "Epoch : 48 completed, train loss : 0.08845065371133387, test loss : 0.2310963124036789\n",
      "Epoch : 49 completed, train loss : 0.08990990929305553, test loss : 0.23005129396915436\n",
      "Epoch : 50 completed, train loss : 0.0849136074539274, test loss : 0.2315441071987152\n",
      "Epoch : 51 completed, train loss : 0.08646658062934875, test loss : 0.23093336820602417\n",
      "Epoch : 52 completed, train loss : 0.08886119350790977, test loss : 0.23324961960315704\n",
      "Epoch : 53 completed, train loss : 0.08461646758951247, test loss : 0.23265214264392853\n",
      "Epoch : 54 completed, train loss : 0.08267095242626965, test loss : 0.2309148907661438\n",
      "Epoch : 55 completed, train loss : 0.08202996733598411, test loss : 0.2329358011484146\n",
      "Epoch : 56 completed, train loss : 0.08247336815111339, test loss : 0.23265309631824493\n",
      "Epoch : 57 completed, train loss : 0.08184412308037281, test loss : 0.23504480719566345\n",
      "Epoch : 58 completed, train loss : 0.07770681683905423, test loss : 0.23869484663009644\n",
      "Epoch : 59 completed, train loss : 0.0806347643956542, test loss : 0.23447179794311523\n",
      "Epoch : 60 completed, train loss : 0.07989110238850117, test loss : 0.23560523986816406\n",
      "Epoch : 61 completed, train loss : 0.07904473342932761, test loss : 0.23786720633506775\n",
      "Epoch : 62 completed, train loss : 0.07480307458899915, test loss : 0.23705266416072845\n",
      "Epoch : 63 completed, train loss : 0.07600676035508513, test loss : 0.23770275712013245\n",
      "Epoch : 64 completed, train loss : 0.07464367407374084, test loss : 0.23941612243652344\n",
      "Epoch : 65 completed, train loss : 0.07441551797091961, test loss : 0.23857411742210388\n",
      "Epoch : 66 completed, train loss : 0.07227487536147237, test loss : 0.23786668479442596\n",
      "Epoch : 67 completed, train loss : 0.0745921260677278, test loss : 0.24011343717575073\n",
      "Epoch : 68 completed, train loss : 0.07349720201455057, test loss : 0.2395731508731842\n",
      "Epoch : 69 completed, train loss : 0.0712512037716806, test loss : 0.24406076967716217\n",
      "Epoch : 70 completed, train loss : 0.07257179752923548, test loss : 0.23843374848365784\n",
      "Epoch : 71 completed, train loss : 0.07111807772889733, test loss : 0.24626675248146057\n",
      "Epoch : 72 completed, train loss : 0.07267741346731782, test loss : 0.23736315965652466\n",
      "Epoch : 73 completed, train loss : 0.07392956060357392, test loss : 0.24306708574295044\n",
      "Epoch : 74 completed, train loss : 0.07040821900591254, test loss : 0.24172964692115784\n",
      "Epoch : 75 completed, train loss : 0.07255416898988187, test loss : 0.24494323134422302\n",
      "Epoch : 76 completed, train loss : 0.0665663001127541, test loss : 0.24352286756038666\n",
      "Epoch : 77 completed, train loss : 0.06981506710872054, test loss : 0.24554754793643951\n",
      "Epoch : 78 completed, train loss : 0.0686580368783325, test loss : 0.24536775052547455\n",
      "Epoch : 79 completed, train loss : 0.06878237961791456, test loss : 0.24994048476219177\n",
      "Epoch : 80 completed, train loss : 0.06979554379358888, test loss : 0.24306000769138336\n",
      "Epoch : 81 completed, train loss : 0.06554978736676276, test loss : 0.2450130581855774\n",
      "Epoch : 82 completed, train loss : 0.06754221674054861, test loss : 0.24261197447776794\n",
      "Epoch : 83 completed, train loss : 0.06718677980825305, test loss : 0.24412645399570465\n",
      "Epoch : 84 completed, train loss : 0.06993279024027288, test loss : 0.24675841629505157\n",
      "Epoch : 85 completed, train loss : 0.06658303854055703, test loss : 0.24635900557041168\n",
      "Epoch : 86 completed, train loss : 0.06864040461368859, test loss : 0.24467572569847107\n",
      "Epoch : 87 completed, train loss : 0.06308103632181883, test loss : 0.2481037676334381\n",
      "Epoch : 88 completed, train loss : 0.06477236025966704, test loss : 0.24537049233913422\n",
      "Epoch : 89 completed, train loss : 0.06278963293880224, test loss : 0.2499067187309265\n",
      "Epoch : 90 completed, train loss : 0.06400096137076616, test loss : 0.2509825825691223\n",
      "Epoch : 91 completed, train loss : 0.05920235067605972, test loss : 0.24613937735557556\n",
      "Epoch : 92 completed, train loss : 0.05987380654551089, test loss : 0.2514053285121918\n",
      "Epoch : 93 completed, train loss : 0.06182814319618046, test loss : 0.24741944670677185\n",
      "Epoch : 94 completed, train loss : 0.060600365279242396, test loss : 0.24812912940979004\n",
      "Epoch : 95 completed, train loss : 0.05979061732068658, test loss : 0.2561894357204437\n",
      "Epoch : 96 completed, train loss : 0.061526888282969594, test loss : 0.24939192831516266\n",
      "Epoch : 97 completed, train loss : 0.059248188976198435, test loss : 0.2482750564813614\n",
      "Epoch : 98 completed, train loss : 0.06025814660824835, test loss : 0.252721905708313\n",
      "Epoch : 99 completed, train loss : 0.05805146018974483, test loss : 0.2502601742744446\n",
      "Epoch : 100 completed, train loss : 0.057479858165606856, test loss : 0.2534940838813782\n",
      "Epoch : 101 completed, train loss : 0.05845721741206944, test loss : 0.25589725375175476\n",
      "Epoch : 102 completed, train loss : 0.059028588933870196, test loss : 0.25048843026161194\n",
      "Epoch : 103 completed, train loss : 0.06039847666397691, test loss : 0.25348204374313354\n",
      "Epoch : 104 completed, train loss : 0.057908369693905115, test loss : 0.25406184792518616\n",
      "Epoch : 105 completed, train loss : 0.060292527079582214, test loss : 0.2517941892147064\n",
      "Epoch : 106 completed, train loss : 0.062427467200905085, test loss : 0.2538517713546753\n",
      "Epoch : 107 completed, train loss : 0.06016157870180905, test loss : 0.2560606300830841\n",
      "Epoch : 108 completed, train loss : 0.0600204651709646, test loss : 0.2530306577682495\n",
      "Epoch : 109 completed, train loss : 0.05991844064556062, test loss : 0.24965982139110565\n",
      "Epoch : 110 completed, train loss : 0.05970817478373647, test loss : 0.2534762918949127\n",
      "Epoch : 111 completed, train loss : 0.059616729617118835, test loss : 0.2537415325641632\n",
      "Epoch : 112 completed, train loss : 0.05764681892469525, test loss : 0.25391289591789246\n",
      "Epoch : 113 completed, train loss : 0.05728934914804995, test loss : 0.25096753239631653\n",
      "Epoch : 114 completed, train loss : 0.05576730938628316, test loss : 0.2547643780708313\n",
      "Epoch : 115 completed, train loss : 0.05560019449330866, test loss : 0.25551891326904297\n",
      "Epoch : 116 completed, train loss : 0.0544474464841187, test loss : 0.2552611529827118\n",
      "Epoch : 117 completed, train loss : 0.059026353526860476, test loss : 0.25883930921554565\n",
      "Epoch : 118 completed, train loss : 0.05548089789226651, test loss : 0.25604912638664246\n",
      "Epoch : 119 completed, train loss : 0.05703983758576214, test loss : 0.25430768728256226\n",
      "Epoch : 120 completed, train loss : 0.05548505997285247, test loss : 0.2569338083267212\n",
      "Epoch : 121 completed, train loss : 0.05555198178626597, test loss : 0.25860586762428284\n",
      "Epoch : 122 completed, train loss : 0.05537319043651223, test loss : 0.25608542561531067\n",
      "Epoch : 123 completed, train loss : 0.05605470226146281, test loss : 0.2561156749725342\n",
      "Epoch : 124 completed, train loss : 0.05417132005095482, test loss : 0.25725314021110535\n",
      "Epoch : 125 completed, train loss : 0.05352228484116495, test loss : 0.2575704753398895\n",
      "Epoch : 126 completed, train loss : 0.05606724228709936, test loss : 0.2588726878166199\n",
      "Epoch : 127 completed, train loss : 0.05368725350126624, test loss : 0.2574422061443329\n",
      "Epoch : 128 completed, train loss : 0.052729475777596235, test loss : 0.2612883746623993\n",
      "Epoch : 129 completed, train loss : 0.055290811927989125, test loss : 0.2594949007034302\n",
      "Epoch : 130 completed, train loss : 0.05415938934311271, test loss : 0.2553486227989197\n",
      "Epoch : 131 completed, train loss : 0.054758714279159904, test loss : 0.25948652625083923\n",
      "Epoch : 132 completed, train loss : 0.05609928723424673, test loss : 0.2567217946052551\n",
      "Epoch : 133 completed, train loss : 0.055841121822595596, test loss : 0.2517348527908325\n",
      "Epoch : 134 completed, train loss : 0.058204589411616325, test loss : 0.2566602826118469\n",
      "Epoch : 135 completed, train loss : 0.05454441090114415, test loss : 0.2595157325267792\n",
      "Epoch : 136 completed, train loss : 0.05441490141674876, test loss : 0.26056379079818726\n",
      "Epoch : 137 completed, train loss : 0.05577577278017998, test loss : 0.2532917261123657\n",
      "Epoch : 138 completed, train loss : 0.05247447919100523, test loss : 0.2597849667072296\n",
      "Epoch : 139 completed, train loss : 0.05361195304431021, test loss : 0.25763407349586487\n",
      "Epoch : 140 completed, train loss : 0.05470552924089134, test loss : 0.2577349841594696\n",
      "Epoch : 141 completed, train loss : 0.05468566692434251, test loss : 0.2600387632846832\n",
      "Epoch : 142 completed, train loss : 0.05614647245965898, test loss : 0.25641632080078125\n",
      "Epoch : 143 completed, train loss : 0.05302517022937536, test loss : 0.25756627321243286\n",
      "Epoch : 144 completed, train loss : 0.05379471636842936, test loss : 0.257443368434906\n",
      "Epoch : 145 completed, train loss : 0.054592630825936794, test loss : 0.25847548246383667\n",
      "Epoch : 146 completed, train loss : 0.05416343570686877, test loss : 0.26062849164009094\n",
      "Epoch : 147 completed, train loss : 0.05429121502675116, test loss : 0.25664636492729187\n",
      "Epoch : 148 completed, train loss : 0.05416327505372465, test loss : 0.2600354254245758\n",
      "Epoch : 149 completed, train loss : 0.05291528254747391, test loss : 0.25884535908699036\n",
      "Epoch : 150 completed, train loss : 0.05426954152062535, test loss : 0.2588016092777252\n",
      "Epoch : 151 completed, train loss : 0.053406261606141925, test loss : 0.25875750184059143\n",
      "Epoch : 152 completed, train loss : 0.05364461778663099, test loss : 0.2577287554740906\n",
      "Epoch : 153 completed, train loss : 0.05312947742640972, test loss : 0.25544965267181396\n",
      "Epoch : 154 completed, train loss : 0.050870389211922884, test loss : 0.2580273747444153\n",
      "Epoch : 155 completed, train loss : 0.05349160358309746, test loss : 0.2576988935470581\n",
      "Epoch : 156 completed, train loss : 0.05280026909895241, test loss : 0.26018640398979187\n",
      "Epoch : 157 completed, train loss : 0.052249190863221884, test loss : 0.2587375044822693\n",
      "Epoch : 158 completed, train loss : 0.05262203956954181, test loss : 0.26011523604393005\n",
      "Epoch : 159 completed, train loss : 0.052571957698091865, test loss : 0.2568409740924835\n",
      "Epoch : 160 completed, train loss : 0.052338312612846494, test loss : 0.2578570544719696\n",
      "Epoch : 161 completed, train loss : 0.052286182064563036, test loss : 0.26020070910453796\n",
      "Epoch : 162 completed, train loss : 0.05143225332722068, test loss : 0.26026055216789246\n",
      "Epoch : 163 completed, train loss : 0.0512587099801749, test loss : 0.25733062624931335\n",
      "Epoch : 164 completed, train loss : 0.05227484577335417, test loss : 0.2567339241504669\n",
      "Epoch : 165 completed, train loss : 0.05282059218734503, test loss : 0.26040399074554443\n",
      "Epoch : 166 completed, train loss : 0.05188306956551969, test loss : 0.25649335980415344\n",
      "Epoch : 167 completed, train loss : 0.05096759553998709, test loss : 0.2572508156299591\n",
      "Epoch : 168 completed, train loss : 0.049512322060763836, test loss : 0.2595989406108856\n",
      "Epoch : 169 completed, train loss : 0.05044559424277395, test loss : 0.2599141299724579\n",
      "Epoch : 170 completed, train loss : 0.051351615227758884, test loss : 0.2606101334095001\n",
      "Epoch : 171 completed, train loss : 0.049447666737250984, test loss : 0.2610277831554413\n",
      "Epoch : 172 completed, train loss : 0.05133684957399964, test loss : 0.26039984822273254\n",
      "Epoch : 173 completed, train loss : 0.05113180843181908, test loss : 0.2594532072544098\n",
      "Epoch : 174 completed, train loss : 0.05164175690151751, test loss : 0.26060521602630615\n",
      "Epoch : 175 completed, train loss : 0.05093201459385455, test loss : 0.263001948595047\n",
      "Epoch : 176 completed, train loss : 0.05187304154969752, test loss : 0.2579543888568878\n",
      "Epoch : 177 completed, train loss : 0.05105055449530482, test loss : 0.2615189850330353\n",
      "Epoch : 178 completed, train loss : 0.04850226710550487, test loss : 0.26019489765167236\n",
      "Epoch : 179 completed, train loss : 0.05146680888719857, test loss : 0.25982043147087097\n",
      "Epoch : 180 completed, train loss : 0.05209171259775758, test loss : 0.2580435574054718\n",
      "Epoch : 181 completed, train loss : 0.051095762522891164, test loss : 0.25626835227012634\n",
      "Epoch : 182 completed, train loss : 0.05020177410915494, test loss : 0.26292917132377625\n",
      "Epoch : 183 completed, train loss : 0.050899104680866, test loss : 0.2603040039539337\n",
      "Epoch : 184 completed, train loss : 0.05047121993266046, test loss : 0.25731807947158813\n",
      "Epoch : 185 completed, train loss : 0.05028579477220774, test loss : 0.25614920258522034\n",
      "Epoch : 186 completed, train loss : 0.0513022281229496, test loss : 0.26290208101272583\n",
      "Epoch : 187 completed, train loss : 0.05090745119377971, test loss : 0.2591542601585388\n",
      "Epoch : 188 completed, train loss : 0.049303804291412234, test loss : 0.2641846835613251\n",
      "Epoch : 189 completed, train loss : 0.05059811670798808, test loss : 0.25970664620399475\n",
      "Epoch : 190 completed, train loss : 0.049566020257771015, test loss : 0.26091843843460083\n",
      "Epoch : 191 completed, train loss : 0.05107902945019305, test loss : 0.2614738643169403\n",
      "Epoch : 192 completed, train loss : 0.05137168196961284, test loss : 0.2613498270511627\n",
      "Epoch : 193 completed, train loss : 0.04993971169460565, test loss : 0.2594904601573944\n",
      "Epoch : 194 completed, train loss : 0.0519717886345461, test loss : 0.26440465450286865\n",
      "Epoch : 195 completed, train loss : 0.04956141416914761, test loss : 0.2632814347743988\n",
      "Epoch : 196 completed, train loss : 0.05003073625266552, test loss : 0.2600610554218292\n",
      "Epoch : 197 completed, train loss : 0.04974194057285786, test loss : 0.2607450783252716\n",
      "Epoch : 198 completed, train loss : 0.04916030075401068, test loss : 0.2595660388469696\n",
      "Epoch : 199 completed, train loss : 0.05017138505354524, test loss : 0.26496633887290955\n",
      "Epoch : 200 completed, train loss : 0.05046509671956301, test loss : 0.261208713054657\n",
      "Epoch : 201 completed, train loss : 0.049307648674584925, test loss : 0.2605093717575073\n",
      "Epoch : 202 completed, train loss : 0.048175735282711685, test loss : 0.26191797852516174\n",
      "Epoch : 203 completed, train loss : 0.049546671216376126, test loss : 0.26408493518829346\n",
      "Epoch : 204 completed, train loss : 0.050728488131426275, test loss : 0.2631104588508606\n",
      "Epoch : 205 completed, train loss : 0.050506387487985194, test loss : 0.260570228099823\n",
      "Epoch : 206 completed, train loss : 0.05041733430698514, test loss : 0.25861769914627075\n",
      "Epoch : 207 completed, train loss : 0.05203155311755836, test loss : 0.2607363760471344\n",
      "Epoch : 208 completed, train loss : 0.0486432418692857, test loss : 0.259591281414032\n",
      "Epoch : 209 completed, train loss : 0.05046379589475691, test loss : 0.261705219745636\n",
      "Epoch : 210 completed, train loss : 0.047551826341077685, test loss : 0.2636941969394684\n",
      "Epoch : 211 completed, train loss : 0.0482994276098907, test loss : 0.26278552412986755\n",
      "Epoch : 212 completed, train loss : 0.048110057367011905, test loss : 0.26297226548194885\n",
      "Epoch : 213 completed, train loss : 0.04982743551954627, test loss : 0.26162102818489075\n",
      "Epoch : 214 completed, train loss : 0.047884633764624596, test loss : 0.2628185451030731\n",
      "Epoch : 215 completed, train loss : 0.05085991765372455, test loss : 0.26230356097221375\n",
      "Epoch : 216 completed, train loss : 0.04966278374195099, test loss : 0.2629421353340149\n",
      "Epoch : 217 completed, train loss : 0.046532450593076646, test loss : 0.2633868157863617\n",
      "Epoch : 218 completed, train loss : 0.051087243016809225, test loss : 0.26176759600639343\n",
      "Epoch : 219 completed, train loss : 0.05114375427365303, test loss : 0.26190608739852905\n",
      "Epoch : 220 completed, train loss : 0.04925749415997416, test loss : 0.26382651925086975\n",
      "Epoch : 221 completed, train loss : 0.04687943961471319, test loss : 0.26594868302345276\n",
      "Epoch : 222 completed, train loss : 0.04964622133411467, test loss : 0.2624605596065521\n",
      "Epoch : 223 completed, train loss : 0.0472784360172227, test loss : 0.26083824038505554\n",
      "Epoch : 224 completed, train loss : 0.04989991569891572, test loss : 0.26039430499076843\n",
      "Epoch : 225 completed, train loss : 0.05036921752616763, test loss : 0.262713223695755\n",
      "Epoch : 226 completed, train loss : 0.049121112329885364, test loss : 0.26058319211006165\n",
      "Epoch : 227 completed, train loss : 0.05119381472468376, test loss : 0.2591419816017151\n",
      "Epoch : 228 completed, train loss : 0.05046792374923825, test loss : 0.26147741079330444\n",
      "Epoch : 229 completed, train loss : 0.0518532139249146, test loss : 0.2637277841567993\n",
      "Epoch : 230 completed, train loss : 0.04897008463740349, test loss : 0.2608126401901245\n",
      "Epoch : 231 completed, train loss : 0.04922968242317438, test loss : 0.2618637979030609\n",
      "Epoch : 232 completed, train loss : 0.05116084334440529, test loss : 0.26103758811950684\n",
      "Epoch : 233 completed, train loss : 0.04531928652431816, test loss : 0.2596750557422638\n",
      "Epoch : 234 completed, train loss : 0.04996523295994848, test loss : 0.26009082794189453\n",
      "Epoch : 235 completed, train loss : 0.04954003309831023, test loss : 0.25895586609840393\n",
      "Epoch : 236 completed, train loss : 0.05172392900567502, test loss : 0.2623917758464813\n",
      "Epoch : 237 completed, train loss : 0.050976130180060863, test loss : 0.2636537551879883\n",
      "Epoch : 238 completed, train loss : 0.0468555229017511, test loss : 0.2651149034500122\n",
      "Epoch : 239 completed, train loss : 0.04716543504036963, test loss : 0.263532429933548\n",
      "Epoch : 240 completed, train loss : 0.046967495465651155, test loss : 0.2593006193637848\n",
      "Epoch : 241 completed, train loss : 0.0485377594595775, test loss : 0.2629255950450897\n",
      "Epoch : 242 completed, train loss : 0.04957453696988523, test loss : 0.26090022921562195\n",
      "Epoch : 243 completed, train loss : 0.04820789839141071, test loss : 0.2613621950149536\n",
      "Epoch : 244 completed, train loss : 0.04860231070779264, test loss : 0.26297780871391296\n",
      "Epoch : 245 completed, train loss : 0.045830659684725106, test loss : 0.25888437032699585\n",
      "Epoch : 246 completed, train loss : 0.05037501337938011, test loss : 0.2625129222869873\n",
      "Epoch : 247 completed, train loss : 0.04801205080002546, test loss : 0.2653229832649231\n",
      "Epoch : 248 completed, train loss : 0.05172812286764383, test loss : 0.2644256353378296\n",
      "Epoch : 249 completed, train loss : 0.04959676507860422, test loss : 0.2627021074295044\n",
      "Epoch : 250 completed, train loss : 0.04871613532304764, test loss : 0.25950223207473755\n",
      "Epoch : 251 completed, train loss : 0.04875115258619189, test loss : 0.25926849246025085\n",
      "Epoch : 252 completed, train loss : 0.0464607032481581, test loss : 0.2613963484764099\n",
      "Epoch : 253 completed, train loss : 0.04749255860224366, test loss : 0.26193392276763916\n",
      "Epoch : 254 completed, train loss : 0.04690913378726691, test loss : 0.2623584568500519\n",
      "Epoch : 255 completed, train loss : 0.046875654836185277, test loss : 0.2618328928947449\n",
      "Epoch : 256 completed, train loss : 0.05155992740765214, test loss : 0.25979262590408325\n",
      "Epoch : 257 completed, train loss : 0.04587799741420895, test loss : 0.2607390582561493\n",
      "Epoch : 258 completed, train loss : 0.04953236086294055, test loss : 0.2601531147956848\n",
      "Epoch : 259 completed, train loss : 0.04820177948568016, test loss : 0.2618541419506073\n",
      "Epoch : 260 completed, train loss : 0.04624615726061165, test loss : 0.2630930244922638\n",
      "Epoch : 261 completed, train loss : 0.0490517639555037, test loss : 0.262112021446228\n",
      "Epoch : 262 completed, train loss : 0.048879291862249374, test loss : 0.2582394778728485\n",
      "Epoch : 263 completed, train loss : 0.04772563313599676, test loss : 0.26218149065971375\n",
      "Epoch : 264 completed, train loss : 0.049715444445610046, test loss : 0.2604497969150543\n",
      "Epoch : 265 completed, train loss : 0.046943090390414, test loss : 0.2598991096019745\n",
      "Epoch : 266 completed, train loss : 0.0490846149623394, test loss : 0.26035505533218384\n",
      "Epoch : 267 completed, train loss : 0.049633964197710156, test loss : 0.2609054148197174\n",
      "Epoch : 268 completed, train loss : 0.0498328129760921, test loss : 0.260779470205307\n",
      "Epoch : 269 completed, train loss : 0.050441828556358814, test loss : 0.2601951062679291\n",
      "Epoch : 270 completed, train loss : 0.04947726894170046, test loss : 0.2588333189487457\n",
      "Epoch : 271 completed, train loss : 0.047515884740278125, test loss : 0.2601826786994934\n",
      "Epoch : 272 completed, train loss : 0.050947604700922966, test loss : 0.25963398814201355\n",
      "Epoch : 273 completed, train loss : 0.048027736716903746, test loss : 0.2595554292201996\n",
      "Epoch : 274 completed, train loss : 0.04806537495460361, test loss : 0.25868168473243713\n",
      "Epoch : 275 completed, train loss : 0.046470098779536784, test loss : 0.25939247012138367\n",
      "Epoch : 276 completed, train loss : 0.04679595539346337, test loss : 0.2602609395980835\n",
      "Epoch : 277 completed, train loss : 0.04731220239773393, test loss : 0.2599797546863556\n",
      "Epoch : 278 completed, train loss : 0.04688884876668453, test loss : 0.25855058431625366\n",
      "Epoch : 279 completed, train loss : 0.047769251046702266, test loss : 0.260625958442688\n",
      "Epoch : 280 completed, train loss : 0.045871291775256395, test loss : 0.2609427571296692\n",
      "Epoch : 281 completed, train loss : 0.04741513030603528, test loss : 0.2623269557952881\n",
      "Epoch : 282 completed, train loss : 0.0494959217030555, test loss : 0.26002395153045654\n",
      "Epoch : 283 completed, train loss : 0.0486833571922034, test loss : 0.26021337509155273\n",
      "Epoch : 284 completed, train loss : 0.048180104116909206, test loss : 0.2594233751296997\n",
      "Epoch : 285 completed, train loss : 0.04736716975457966, test loss : 0.2616010904312134\n",
      "Epoch : 286 completed, train loss : 0.04779062618035823, test loss : 0.26122575998306274\n",
      "Epoch : 287 completed, train loss : 0.045920151635073125, test loss : 0.25679466128349304\n",
      "Epoch : 288 completed, train loss : 0.048449897207319736, test loss : 0.26200398802757263\n",
      "Epoch : 289 completed, train loss : 0.04745606833603233, test loss : 0.2587985694408417\n",
      "Epoch : 290 completed, train loss : 0.0469380347058177, test loss : 0.25885769724845886\n",
      "Epoch : 291 completed, train loss : 0.04721444449387491, test loss : 0.2603549361228943\n",
      "Epoch : 292 completed, train loss : 0.04740065650548786, test loss : 0.2585656940937042\n",
      "Epoch : 293 completed, train loss : 0.04733228939585388, test loss : 0.25784870982170105\n",
      "Epoch : 294 completed, train loss : 0.048694191966205835, test loss : 0.25987282395362854\n",
      "Epoch : 295 completed, train loss : 0.048045741161331534, test loss : 0.25939494371414185\n",
      "Epoch : 296 completed, train loss : 0.045560753089375794, test loss : 0.2587863802909851\n",
      "Epoch : 297 completed, train loss : 0.04705832211766392, test loss : 0.25848186016082764\n",
      "Epoch : 298 completed, train loss : 0.04525417182594538, test loss : 0.25883200764656067\n",
      "Epoch : 299 completed, train loss : 0.04833515372592956, test loss : 0.2595288157463074\n",
      "Epoch : 300 completed, train loss : 0.047653703950345516, test loss : 0.2597403824329376\n",
      "Epoch : 301 completed, train loss : 0.04782788292504847, test loss : 0.25793614983558655\n",
      "Epoch : 302 completed, train loss : 0.049747862736694515, test loss : 0.2582710087299347\n",
      "Epoch : 303 completed, train loss : 0.04874273727182299, test loss : 0.2591576874256134\n",
      "Epoch : 304 completed, train loss : 0.050055820029228926, test loss : 0.26011401414871216\n",
      "Epoch : 305 completed, train loss : 0.04778969578910619, test loss : 0.25857260823249817\n",
      "Epoch : 306 completed, train loss : 0.046907017822377384, test loss : 0.25884392857551575\n",
      "Epoch : 307 completed, train loss : 0.048227302031591535, test loss : 0.2589993178844452\n",
      "Epoch : 308 completed, train loss : 0.04731612361501902, test loss : 0.2594885230064392\n",
      "Epoch : 309 completed, train loss : 0.048256150214001536, test loss : 0.2571166753768921\n",
      "Epoch : 310 completed, train loss : 0.047291134716942906, test loss : 0.2589019536972046\n",
      "Epoch : 311 completed, train loss : 0.04670406808145344, test loss : 0.25931215286254883\n",
      "Epoch : 312 completed, train loss : 0.04568232339806855, test loss : 0.25671544671058655\n",
      "Epoch : 313 completed, train loss : 0.04805130045861006, test loss : 0.2583836615085602\n",
      "Epoch : 314 completed, train loss : 0.04893562546931207, test loss : 0.2588456869125366\n",
      "Epoch : 315 completed, train loss : 0.049945918610319495, test loss : 0.2587445080280304\n",
      "Epoch : 316 completed, train loss : 0.04801568167749792, test loss : 0.25786063075065613\n",
      "Epoch : 317 completed, train loss : 0.04784774675499648, test loss : 0.2616182565689087\n",
      "Epoch : 318 completed, train loss : 0.04712521890178323, test loss : 0.25826379656791687\n",
      "Epoch : 319 completed, train loss : 0.04855725262314081, test loss : 0.2578894793987274\n",
      "Epoch : 320 completed, train loss : 0.04793309618253261, test loss : 0.2615358233451843\n",
      "Epoch : 321 completed, train loss : 0.04537474096287042, test loss : 0.2597856819629669\n",
      "Epoch : 322 completed, train loss : 0.045584571082144976, test loss : 0.25754985213279724\n",
      "Epoch : 323 completed, train loss : 0.04805933823809028, test loss : 0.2583811283111572\n",
      "Epoch : 324 completed, train loss : 0.04900916072074324, test loss : 0.2600763440132141\n",
      "Epoch : 325 completed, train loss : 0.046413304982706904, test loss : 0.25921687483787537\n",
      "Epoch : 326 completed, train loss : 0.04628090909682214, test loss : 0.26103994250297546\n",
      "Epoch : 327 completed, train loss : 0.04907281487248838, test loss : 0.25943830609321594\n",
      "Epoch : 328 completed, train loss : 0.04922146536409855, test loss : 0.2571454346179962\n",
      "Epoch : 329 completed, train loss : 0.047451892984099686, test loss : 0.25829288363456726\n",
      "Epoch : 330 completed, train loss : 0.04793720436282456, test loss : 0.25824928283691406\n",
      "Epoch : 331 completed, train loss : 0.04839411342982203, test loss : 0.25611284375190735\n",
      "Epoch : 332 completed, train loss : 0.04964348813518882, test loss : 0.2572992742061615\n",
      "Epoch : 333 completed, train loss : 0.04554146737791598, test loss : 0.26023226976394653\n",
      "Epoch : 334 completed, train loss : 0.04726370808202773, test loss : 0.258582204580307\n",
      "Epoch : 335 completed, train loss : 0.04814541153609753, test loss : 0.25753700733184814\n",
      "Epoch : 336 completed, train loss : 0.04631671286188066, test loss : 0.2575017213821411\n",
      "Epoch : 337 completed, train loss : 0.046570598846301436, test loss : 0.25803062319755554\n",
      "Epoch : 338 completed, train loss : 0.04758875072002411, test loss : 0.25975367426872253\n",
      "Epoch : 339 completed, train loss : 0.04637321678455919, test loss : 0.25913146138191223\n",
      "Epoch : 340 completed, train loss : 0.044968146714381874, test loss : 0.2564904987812042\n",
      "Epoch : 341 completed, train loss : 0.04691172484308481, test loss : 0.2599736452102661\n",
      "Epoch : 342 completed, train loss : 0.04742489382624626, test loss : 0.25863388180732727\n",
      "Epoch : 343 completed, train loss : 0.04595630569383502, test loss : 0.25662967562675476\n",
      "Epoch : 344 completed, train loss : 0.047059744712896645, test loss : 0.25810563564300537\n",
      "Epoch : 345 completed, train loss : 0.044492964167147875, test loss : 0.2584884762763977\n",
      "Epoch : 346 completed, train loss : 0.047056294861249626, test loss : 0.2582753300666809\n",
      "Epoch : 347 completed, train loss : 0.04805516288615763, test loss : 0.2582108676433563\n",
      "Epoch : 348 completed, train loss : 0.04720039700623602, test loss : 0.25986018776893616\n",
      "Epoch : 349 completed, train loss : 0.04713934555184096, test loss : 0.2584567368030548\n",
      "Epoch : 350 completed, train loss : 0.046427378547377884, test loss : 0.2598519027233124\n",
      "Epoch : 351 completed, train loss : 0.04673758486751467, test loss : 0.25828373432159424\n",
      "Epoch : 352 completed, train loss : 0.04502645833417773, test loss : 0.2582731544971466\n",
      "Epoch : 353 completed, train loss : 0.04776365379802883, test loss : 0.2555578649044037\n",
      "Epoch : 354 completed, train loss : 0.04674895980861038, test loss : 0.25548118352890015\n",
      "Epoch : 355 completed, train loss : 0.04945187480188906, test loss : 0.25543835759162903\n",
      "Epoch : 356 completed, train loss : 0.04689980624243617, test loss : 0.25762036442756653\n",
      "Epoch : 357 completed, train loss : 0.0466389290522784, test loss : 0.2578076124191284\n",
      "Epoch : 358 completed, train loss : 0.048446795786730945, test loss : 0.2568880021572113\n",
      "Epoch : 359 completed, train loss : 0.0484623615629971, test loss : 0.2563828229904175\n",
      "Epoch : 360 completed, train loss : 0.04986606421880424, test loss : 0.25778982043266296\n",
      "Epoch : 361 completed, train loss : 0.04695165413431823, test loss : 0.25608235597610474\n",
      "Epoch : 362 completed, train loss : 0.04785664891824126, test loss : 0.2549505829811096\n",
      "Epoch : 363 completed, train loss : 0.046861903043463826, test loss : 0.25359323620796204\n",
      "Epoch : 364 completed, train loss : 0.04968307539820671, test loss : 0.2573249638080597\n",
      "Epoch : 365 completed, train loss : 0.050163710955530405, test loss : 0.2529682219028473\n",
      "Epoch : 366 completed, train loss : 0.04750779923051596, test loss : 0.2558838427066803\n",
      "Epoch : 367 completed, train loss : 0.048268041224218905, test loss : 0.2579425573348999\n",
      "Epoch : 368 completed, train loss : 0.046345632639713585, test loss : 0.2561207115650177\n",
      "Epoch : 369 completed, train loss : 0.04713570687454194, test loss : 0.2561531960964203\n",
      "Epoch : 370 completed, train loss : 0.046846843673847616, test loss : 0.25711891055107117\n",
      "Epoch : 371 completed, train loss : 0.04646481922827661, test loss : 0.2555614411830902\n",
      "Epoch : 372 completed, train loss : 0.04702503327280283, test loss : 0.2540428340435028\n",
      "Epoch : 373 completed, train loss : 0.04807688505388796, test loss : 0.25768736004829407\n",
      "Epoch : 374 completed, train loss : 0.04732555500231683, test loss : 0.2579808235168457\n",
      "Epoch : 375 completed, train loss : 0.044745577266439795, test loss : 0.2591574788093567\n",
      "Epoch : 376 completed, train loss : 0.046502723009325564, test loss : 0.25730106234550476\n",
      "Epoch : 377 completed, train loss : 0.04664709826465696, test loss : 0.2533343434333801\n",
      "Epoch : 378 completed, train loss : 0.04689815093297511, test loss : 0.2591291069984436\n",
      "Epoch : 379 completed, train loss : 0.04679888882674277, test loss : 0.256033331155777\n",
      "Epoch : 380 completed, train loss : 0.04807556013111025, test loss : 0.2543337941169739\n",
      "Epoch : 381 completed, train loss : 0.048186135245487094, test loss : 0.257285475730896\n",
      "Epoch : 382 completed, train loss : 0.04783034848514944, test loss : 0.25723400712013245\n",
      "Epoch : 383 completed, train loss : 0.045025013852864504, test loss : 0.25809240341186523\n",
      "Epoch : 384 completed, train loss : 0.04468625760637224, test loss : 0.2552533447742462\n",
      "Epoch : 385 completed, train loss : 0.048323851893655956, test loss : 0.25720927119255066\n",
      "Epoch : 386 completed, train loss : 0.04562546906527132, test loss : 0.2572469711303711\n",
      "Epoch : 387 completed, train loss : 0.044006881886161864, test loss : 0.25777962803840637\n",
      "Epoch : 388 completed, train loss : 0.04576229827944189, test loss : 0.2555123567581177\n",
      "Epoch : 389 completed, train loss : 0.04830924514681101, test loss : 0.2547696828842163\n",
      "Epoch : 390 completed, train loss : 0.04626555705908686, test loss : 0.2566109597682953\n",
      "Epoch : 391 completed, train loss : 0.0449191986117512, test loss : 0.25604191422462463\n",
      "Epoch : 392 completed, train loss : 0.04725797439459711, test loss : 0.25769156217575073\n",
      "Epoch : 393 completed, train loss : 0.0467509861337021, test loss : 0.25575196743011475\n",
      "Epoch : 394 completed, train loss : 0.048039018642157316, test loss : 0.25794145464897156\n",
      "Epoch : 395 completed, train loss : 0.046343446476385, test loss : 0.2565617263317108\n",
      "Epoch : 396 completed, train loss : 0.045476132771000266, test loss : 0.25272905826568604\n",
      "Epoch : 397 completed, train loss : 0.04820825404021889, test loss : 0.25481805205345154\n",
      "Epoch : 398 completed, train loss : 0.04971837694756687, test loss : 0.25478479266166687\n",
      "Epoch : 399 completed, train loss : 0.049965126905590296, test loss : 0.2567348778247833\n",
      "Epoch : 400 completed, train loss : 0.046785053447820246, test loss : 0.25387370586395264\n",
      "Epoch : 401 completed, train loss : 0.04679855552967638, test loss : 0.25584957003593445\n",
      "Epoch : 402 completed, train loss : 0.04688300006091595, test loss : 0.25555479526519775\n",
      "Epoch : 403 completed, train loss : 0.04899263079278171, test loss : 0.25276562571525574\n",
      "Epoch : 404 completed, train loss : 0.04677100316621363, test loss : 0.2545889616012573\n",
      "Epoch : 405 completed, train loss : 0.04769680800382048, test loss : 0.2563168406486511\n",
      "Epoch : 406 completed, train loss : 0.04751718160696328, test loss : 0.2548777163028717\n",
      "Epoch : 407 completed, train loss : 0.04742982122115791, test loss : 0.253840833902359\n",
      "Epoch : 408 completed, train loss : 0.04539891937747598, test loss : 0.25607001781463623\n",
      "Epoch : 409 completed, train loss : 0.04770223586820066, test loss : 0.2537231147289276\n",
      "Epoch : 410 completed, train loss : 0.048121968284249306, test loss : 0.25414717197418213\n",
      "Epoch : 411 completed, train loss : 0.04806517553515732, test loss : 0.2570706903934479\n",
      "Epoch : 412 completed, train loss : 0.04617384169250727, test loss : 0.25375139713287354\n",
      "Epoch : 413 completed, train loss : 0.0472491008695215, test loss : 0.2540340721607208\n",
      "Epoch : 414 completed, train loss : 0.046208842541091144, test loss : 0.25390979647636414\n",
      "Epoch : 415 completed, train loss : 0.04551981424447149, test loss : 0.25347375869750977\n",
      "Epoch : 416 completed, train loss : 0.04605344252195209, test loss : 0.25561898946762085\n",
      "Epoch : 417 completed, train loss : 0.04813923442270607, test loss : 0.25371503829956055\n",
      "Epoch : 418 completed, train loss : 0.047755832434631884, test loss : 0.2540214955806732\n",
      "Epoch : 419 completed, train loss : 0.04501640354283154, test loss : 0.2539721727371216\n",
      "Epoch : 420 completed, train loss : 0.046379623119719326, test loss : 0.25256675481796265\n",
      "Epoch : 421 completed, train loss : 0.0489967301255092, test loss : 0.2540532350540161\n",
      "Epoch : 422 completed, train loss : 0.0461690635420382, test loss : 0.2556835412979126\n",
      "Epoch : 423 completed, train loss : 0.04726181132718921, test loss : 0.25384974479675293\n",
      "Epoch : 424 completed, train loss : 0.04818576399702579, test loss : 0.2533157169818878\n",
      "Epoch : 425 completed, train loss : 0.045760607230477035, test loss : 0.25470900535583496\n",
      "Epoch : 426 completed, train loss : 0.047282732324674726, test loss : 0.2548442780971527\n",
      "Epoch : 427 completed, train loss : 0.04450159624684602, test loss : 0.2559885084629059\n",
      "Epoch : 428 completed, train loss : 0.043896149727515876, test loss : 0.2551077604293823\n",
      "Epoch : 429 completed, train loss : 0.04604721907526255, test loss : 0.2539510428905487\n",
      "Epoch : 430 completed, train loss : 0.04531438532285392, test loss : 0.25438687205314636\n",
      "Epoch : 431 completed, train loss : 0.046296517248265445, test loss : 0.25464150309562683\n",
      "Epoch : 432 completed, train loss : 0.04715076717548072, test loss : 0.2527288794517517\n",
      "Epoch : 433 completed, train loss : 0.04774085327517241, test loss : 0.254492849111557\n",
      "Epoch : 434 completed, train loss : 0.0463810550281778, test loss : 0.25507593154907227\n",
      "Epoch : 435 completed, train loss : 0.043885900638997555, test loss : 0.25363481044769287\n",
      "Epoch : 436 completed, train loss : 0.04609985707793385, test loss : 0.25372180342674255\n",
      "Epoch : 437 completed, train loss : 0.04546520672738552, test loss : 0.2544500231742859\n",
      "Epoch : 438 completed, train loss : 0.04586125467903912, test loss : 0.25532057881355286\n",
      "Epoch : 439 completed, train loss : 0.04724338045343757, test loss : 0.2548964321613312\n",
      "Epoch : 440 completed, train loss : 0.04903029301203787, test loss : 0.2539599537849426\n",
      "Epoch : 441 completed, train loss : 0.04730821982957423, test loss : 0.25562751293182373\n",
      "Epoch : 442 completed, train loss : 0.04725452046841383, test loss : 0.25061100721359253\n",
      "Epoch : 443 completed, train loss : 0.04648766212631017, test loss : 0.24958133697509766\n",
      "Epoch : 444 completed, train loss : 0.04845307697542012, test loss : 0.25053709745407104\n",
      "Epoch : 445 completed, train loss : 0.04549586237408221, test loss : 0.24996384978294373\n",
      "Epoch : 446 completed, train loss : 0.04548705113120377, test loss : 0.25168782472610474\n",
      "Epoch : 447 completed, train loss : 0.05012111540418118, test loss : 0.25290051102638245\n",
      "Epoch : 448 completed, train loss : 0.047269602539017797, test loss : 0.2521488666534424\n",
      "Epoch : 449 completed, train loss : 0.04591521178372204, test loss : 0.25262072682380676\n",
      "Epoch : 450 completed, train loss : 0.0465059558628127, test loss : 0.253812700510025\n",
      "Epoch : 451 completed, train loss : 0.046716017299331725, test loss : 0.2541502118110657\n",
      "Epoch : 452 completed, train loss : 0.049162142211571336, test loss : 0.2513751983642578\n",
      "Epoch : 453 completed, train loss : 0.047434887499548495, test loss : 0.25256821513175964\n",
      "Epoch : 454 completed, train loss : 0.04605220933444798, test loss : 0.25165730714797974\n",
      "Epoch : 455 completed, train loss : 0.04657900473102927, test loss : 0.2522883415222168\n",
      "Epoch : 456 completed, train loss : 0.04848422890063375, test loss : 0.2507968246936798\n",
      "Epoch : 457 completed, train loss : 0.04688018362503499, test loss : 0.2522996962070465\n",
      "Epoch : 458 completed, train loss : 0.04566514759790152, test loss : 0.25314804911613464\n",
      "Epoch : 459 completed, train loss : 0.04776860470883548, test loss : 0.2535696029663086\n",
      "Epoch : 460 completed, train loss : 0.04772787471301854, test loss : 0.25216397643089294\n",
      "Epoch : 461 completed, train loss : 0.0451698157703504, test loss : 0.2547141909599304\n",
      "Epoch : 462 completed, train loss : 0.04493339848704636, test loss : 0.25217050313949585\n",
      "Epoch : 463 completed, train loss : 0.04542772541753948, test loss : 0.2513255476951599\n",
      "Epoch : 464 completed, train loss : 0.046329806791618466, test loss : 0.25063714385032654\n",
      "Epoch : 465 completed, train loss : 0.047438021400012076, test loss : 0.25156012177467346\n",
      "Epoch : 466 completed, train loss : 0.046161605743691325, test loss : 0.2517056167125702\n",
      "Epoch : 467 completed, train loss : 0.04489996132906526, test loss : 0.2515023648738861\n",
      "Epoch : 468 completed, train loss : 0.04837756953202188, test loss : 0.25088268518447876\n",
      "Epoch : 469 completed, train loss : 0.0456925363978371, test loss : 0.25255656242370605\n",
      "Epoch : 470 completed, train loss : 0.04577593656722456, test loss : 0.2518024742603302\n",
      "Epoch : 471 completed, train loss : 0.04791018820833415, test loss : 0.25145864486694336\n",
      "Epoch : 472 completed, train loss : 0.044996465439908206, test loss : 0.25033608078956604\n",
      "Epoch : 473 completed, train loss : 0.046547667821869254, test loss : 0.2529359757900238\n",
      "Epoch : 474 completed, train loss : 0.04525732505135238, test loss : 0.2512546479701996\n",
      "Epoch : 475 completed, train loss : 0.04650281323119998, test loss : 0.2519749402999878\n",
      "Epoch : 476 completed, train loss : 0.04539280210155994, test loss : 0.24973097443580627\n",
      "Epoch : 477 completed, train loss : 0.048479663440957665, test loss : 0.2507820427417755\n",
      "Epoch : 478 completed, train loss : 0.044846421806141734, test loss : 0.254334419965744\n",
      "Epoch : 479 completed, train loss : 0.04729642788879573, test loss : 0.2516135573387146\n",
      "Epoch : 480 completed, train loss : 0.047862017061561346, test loss : 0.25125667452812195\n",
      "Epoch : 481 completed, train loss : 0.047854919102974236, test loss : 0.251713365316391\n",
      "Epoch : 482 completed, train loss : 0.048570751561783254, test loss : 0.2524201273918152\n",
      "Epoch : 483 completed, train loss : 0.04664750408846885, test loss : 0.25268426537513733\n",
      "Epoch : 484 completed, train loss : 0.04575857613235712, test loss : 0.25316229462623596\n",
      "Epoch : 485 completed, train loss : 0.045535010285675526, test loss : 0.2524917423725128\n",
      "Epoch : 486 completed, train loss : 0.04657747829332948, test loss : 0.2505834996700287\n",
      "Epoch : 487 completed, train loss : 0.04726478178054094, test loss : 0.25131651759147644\n",
      "Epoch : 488 completed, train loss : 0.04601192497648299, test loss : 0.25116100907325745\n",
      "Epoch : 489 completed, train loss : 0.04377510363701731, test loss : 0.25485655665397644\n",
      "Epoch : 490 completed, train loss : 0.046143317711539567, test loss : 0.2503766715526581\n",
      "Epoch : 491 completed, train loss : 0.04509310866706073, test loss : 0.2511436641216278\n",
      "Epoch : 492 completed, train loss : 0.04722340835724026, test loss : 0.2529012858867645\n",
      "Epoch : 493 completed, train loss : 0.048345744027756155, test loss : 0.25020214915275574\n",
      "Epoch : 494 completed, train loss : 0.04486076673492789, test loss : 0.25295135378837585\n",
      "Epoch : 495 completed, train loss : 0.04820517951156944, test loss : 0.25265905261039734\n",
      "Epoch : 496 completed, train loss : 0.04578967054840177, test loss : 0.24915872514247894\n",
      "Epoch : 497 completed, train loss : 0.04651151259895414, test loss : 0.25254809856414795\n",
      "Epoch : 498 completed, train loss : 0.04636329470667988, test loss : 0.25243139266967773\n",
      "Epoch : 499 completed, train loss : 0.045743773807771504, test loss : 0.25150153040885925\n"
     ]
    }
   ],
   "source": [
    "L,Lt = model.pretrain(X,y,X_test,y_test,max_iter=500,lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2114d219e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L, label = 'Train')\n",
    "plt.plot(Lt, label = 'Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_board = Chess_env()\n",
    "\n",
    "chess_board.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_board.board.push_san('Bg5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_board.board_feat.translate(chess_board.board.fen())\n",
    "\n",
    "chess_board.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = chess_board._next_observation()\n",
    "\n",
    "inp = convert_s_to_tensor([inp])\n",
    "\n",
    "preds = model.model.predict(inp)\n",
    "print(preds)\n",
    "print(np.argmax(preds[0]))\n",
    "print(chess_board.board.legal_moves)\n",
    "list(chess_board.board.legal_moves)[np.argmax(preds[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def convert_s_to_tensor(state):\n",
    "    out = []\n",
    "    for i in range(len(state[0])):\n",
    "        state_array = np.array([state[j][i] for j in range(len(state))])\n",
    "        out.append(tf.convert_to_tensor(state_array))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*2*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19a9e2250981a50c8d3a3d12972bc8b4095913f61c1fe8c3c9c8b6c0d63cf46a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
